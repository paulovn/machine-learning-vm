# Environment variables to add to all Spark processes
# ---------------------------------------------------

# In local mode we do not want set Hadoop config, else the default FS 
# would point to HDFS
unset HADOOP_CONF_DIR
#HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-/opt/spark/current/conf/hadoop}

# This enables netlib-java to use the optimized BLAS libraries
# For this to work, the Spark distribution needs to contain the native reference
# implementation (which binary pre-built packages do not usually include)
LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libopenblas.so

# Set the Python to use for executors
PYSPARK_PYTHON=/opt/ipnb/bin/python

# Set Python for Driver -- but only if not set (to allow Jupyter notebook to run)
PYSPARK_DRIVER_PYTHON=${PYSPARK_DRIVER_PYTHON:-/opt/ipnb/bin/ipython}

# Default arguments for job submission
PYSPARK_SUBMIT_ARGS='--master local[*] --driver-memory 1536M --num-executors 2 --executor-cores 2 --executor-memory 1g'

# ------------------------------------------------------------------------------
# To be able to use a local GraphFrames JAR in Python, enlarge PYTHONPATH with
# the GraphFrames path
#GFL#PYTHONPATH=${PYTHONPATH:+$PYTHONPATH:}/opt/spark/current/jars/extra/graphframes_2.11-0.6.0-spark2.3.jar
# ------------------------------------------------------------------------------
