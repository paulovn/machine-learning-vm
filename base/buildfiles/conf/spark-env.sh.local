# Environment variables to add to all Spark processes
# ---------------------------------------------------

# In local mode we do not want set Hadoop config, else the default FS 
# would point to HDFS
unset HADOOP_CONF_DIR
#HADOOP_CONF_DIR=${HADOOP_CONF_DIR:-/opt/spark/current/conf/hadoop}

# Configuration for the optimized BLAS libraries
# (the Spark distribution needs to contain the native reference
# implementation, which binary pre-built packages do not usually include)
LOPENBLAS_NUM_THREADS=1

# Set the Python to use for executors
PYSPARK_PYTHON=/opt/ipnb/bin/python

# Set Python for Driver - but only if not set (to allow Jupyter notebook to run)
PYSPARK_DRIVER_PYTHON=${PYSPARK_DRIVER_PYTHON:-/opt/ipnb/bin/ipython}

# Default arguments for job submission
PYSPARK_SUBMIT_ARGS='--master local[*] --driver-memory 1536M --num-executors 2 --executor-cores 2 --executor-memory 1g'

# For Arrow 2.0.0
PYARROW_IGNORE_TIMEZONE=1

# ------------------------------------------------------------------------------
# To be able to use GraphFrames in Python with a local JAR, enlarge PYTHONPATH
# with the GraphFrames path
#GFL#PYTHONPATH=${PYTHONPATH:+$PYTHONPATH:}/opt/spark/current/jars/extra/graphframes_2.12-0.8.1-spark3.0.jar
# ------------------------------------------------------------------------------
