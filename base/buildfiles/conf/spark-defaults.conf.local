# A basic Spark default configuration, for local execution
# --------------------------------------------------------

# Set execution mode as local, multithread
spark.master=local[*]

# Activate logging
spark.eventLog.enabled=true
spark.eventLog.dir=/var/log/ipnb

# Optimize Pandas conversion using Arrow
spark.sql.execution.arrow.enabled=true
# Lower this limit in case of out of memory problems
#spark.sql.execution.arrow.maxRecordsPerBatch=10000

# ******************************************************************************
# Extra packages to add for additional functionality.
# Uncomment the desired option, by removing the "#xxx#" prefix

# (options in spark.jars.packages can be combined: separate packages with commas)


# ------------------------------------------------------------------------------
# GraphFrames. Two modalities:

# [1] declare the package so that it is downloaded automatically together with
# its dependencies. Of course, it needs to be available in the public repository

#GFP#spark.jars.packages=graphframes:graphframes:0.7.0-spark2.4-s_2.11

# [2] point to a locally downloaded copy, and declare also its dependencies

#GFL#spark.jars=/opt/spark/current/jars/extra/graphframes_2.11-0.7.0-spark2.4.jar
#GFL#spark.jars.packages=org.slf4j:slf4j-api:1.7.25

# ... and to use it in Python, don't forget to add it in spark-env.sh

# ------------------------------------------------------------------------------
# Kafka
# And this if we need Kafka integration in Spark Streaming. There are two versions:

# [1] "Classic" streaming. Using Kafka 0.8 (since the 0.10 version w/ the new
# consumer API does not work in Python)

#KFC#spark.jars.packages=org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.0

# [2] Structured streaming

#KFS#spark.jars.packages=org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0
