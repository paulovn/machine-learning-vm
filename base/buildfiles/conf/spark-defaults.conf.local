# A basic Spark default configuration, for local execution
# --------------------------------------------------------

# Set execution mode as local, multithread
spark.master=local[*]

# Activate logging
spark.eventLog.enabled=true
spark.eventLog.dir=/var/log/ipnb

# ******************************************************************************

# Optimize Pandas conversion using Arrow
spark.sql.execution.arrow.enabled=true
# Lower this limit in case of out of memory problems
#spark.sql.execution.arrow.maxRecordsPerBatch=10000

# Use the optimized version of Hadoop FileOutputCommiter to write output files
# (otherwise we could get race conditions on slow filesystems, such as a shared folder)
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2

# ------------------------------------------------------------------------------
# Brotli
# To enable reading/writing Parquet files compressed with Brotli we need
# to add the Hadoop Brotli codec to Spark

#spark.driver.extraClassPath=/opt/spark/extra/brotli-codec-0.1.1.jar
#spark.executor.extraClassPath=/opt/spark/extra/brotli-codec-0.1.1.jar

# These variants (for automatic package download) do not seem to work
##spark.jars.packages=com.github.rdblue:brotli-codec:0.1.1
##spark.jars.repositories=https://repository.mulesoft.org/nexus/content/repositories/public/
##spark.jars.repositories=https://jitpack.io


# ******************************************************************************
# Extra packages to add for additional functionality.
# Uncomment the desired option, by removing the "#xxx#" prefix

# (options in spark.jars.packages can be combined: separate packages with commas)


# ------------------------------------------------------------------------------
# GraphFrames. Two modalities:

# [1] declare the package so that it is downloaded automatically together with
# its dependencies. Of course, it needs to be available in the public repository

#GFP#spark.jars.packages=graphframes:graphframes:0.8.0-spark2.4-s_2.11

# [2] point to a locally downloaded copy, and declare also its dependencies

#GFL#spark.jars=/opt/spark/current/jars/extra/graphframes_2.11-0.8.0-spark2.4.jar
#GFL#spark.jars.packages=org.slf4j:slf4j-api:1.7.25

# ... and for [2], to use it in Python, don't forget to add it in spark-env.sh

# ------------------------------------------------------------------------------
# Kafka
# This if we need Kafka integration in Spark Streaming. There are two versions:

# [1] "Classic" streaming. Using Kafka 0.8 (since the 0.10 version w/ the new
# consumer API does not work in Python)

#KFC#spark.jars.packages=org.apache.spark:spark-streaming-kafka-0-8_2.11:2.4.6

# [2] Structured streaming

#KFS#spark.jars.packages=org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.6

