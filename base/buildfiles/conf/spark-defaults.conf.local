# A basic Spark default configuration, for local execution
# --------------------------------------------------------

# Set execution mode as local, multithread
spark.master=local[*]

# Activate logging
spark.eventLog.enabled=true
spark.eventLog.dir=/var/log/ipnb


# ******************************************************************************
# Extra packages to add for additional functionality.
# Uncomment the desired option, by removing the "#xxx#" prefix

# (options in spark.jars.packages can be combined: separate packages with commas)


# ------------------------------------------------------------------------------
# GraphFrames. Two modalities:

# [1] declare the package so that it is downloaded automatically together with
# its dependencies. Of course, it needs to be available

#GFP#spark.jars.packages=graphframes:graphframes:0.6.0-spark2.3-s_2.11

# [2] point to a locally downloaded copy, and declare also its dependencies

#GFL#spark.jars=/opt/spark/current/jars/extra/graphframes_2.11-0.6.0-spark2.3.jar
#GFL#spark.jars.packages=com.typesafe.scala-logging:scala-logging-slf4j_2.11:2.1.2,com.typesafe.scala-logging:scala-logging-api_2.11:2.1.2

# ... and to use it in Python, don't forget to add it in spark-env.sh

# ------------------------------------------------------------------------------
# Kafka
# And this if we need Kafka integration in Spark Streaming. There are two versions:

# [1] "Classic" streaming. Using Kafka 0.8 (since the 0.10 version w/ the new
# consumer API does not work in Python)

#KFC#spark.jars.packages=org.apache.spark:spark-streaming-kafka-0-8_2.11:2.3.2

# [2] Structured streaming

#KFS#spark.jars.packages=org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.2
