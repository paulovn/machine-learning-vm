#!/bin/bash
#
# Starts a Spark notebook
#
# chkconfig: 345 98 02
# description: Spark notebook process
#
### BEGIN INIT INFO
# Provides:          ipython-notebook
# Short-Description: IPython notebook
# Default-Start:     3 4 5
# Default-Stop:      0 1 2 6
# Required-Start:    $syslog $remote_fs
# Required-Stop:     $syslog $remote_fs
# Should-Start:
# Should-Stop:
### END INIT INFO

# Source function library.                                     
. /lib/lsb/init-functions

# Slurp the Spark notebook configuration
CFGNAME=/etc/sysconfig/ipython-notebook
set -a
test -f ${CFGNAME} && . ${CFGNAME}
set +a
NOTEBOOK_USER=${NOTEBOOK_USER:-spark}
NOTEBOOK_SCRIPT=${NOTEBOOK_SCRIPT:-/home/${NOTEBOOK_USER}/bin/pyspark-nb}


DESC="IPython Notebook"
DAEMON="ipython-notebook"
VARDIR="/var/run/ipython"
PIDFILE="$VARDIR/$DAEMON.pid"
LOCKDIR="/var/lock/subsys"
LOCKFILE="$LOCKDIR/$DAEMON"

RETVAL_SUCCESS=0

STATUS_RUNNING=0
STATUS_DEAD=1
STATUS_DEAD_AND_LOCK=2
STATUS_NOT_RUNNING=3
STATUS_OTHER_ERROR=102

RETVAL=0
SLEEP_TIME=5
PROC_NAME=$(basename $NOTEBOOK_SCRIPT)

EXEC_PATH=$NOTEBOOK_SCRIPT
EXEC_DIR=""
DAEMON_FLAGS=""

[ -d "$VARDIR" ]  || install -d -m 0755 -o $NOTEBOOK_USER -g $NOTEBOOK_USER $VARDIR 1>/dev/null 2>&1 || :
[ -d "$LOCKDIR" ] || install -d -m 0755 $LOCKDIR 1>/dev/null 2>&1 || :


start() {
    [ -d /home/$NOTEBOOK_USER/IPNB ] || { log_failure_msg "Error: notebook dir not present"; return 1; } 
    [ -x $EXE_FILE ] || exit $ERROR_PROGRAM_NOT_INSTALLED
    log_success_msg "Starting $DESC (${DAEMON}): "

    checkstatusofproc
    status=$?
    if [ "$status" -eq "$STATUS_RUNNING" ]; then
        log_success_msg "${DESC} is running"
        exit 0
    fi

    LOG_FILE=/var/log/spark/${DAEMON}.out
    echo "************ Start on $(date)" >> ${LOG_FILE}
    chown $NOTEBOOK_USER ${LOG_FILE}
    runuser -s /bin/bash $NOTEBOOK_USER -c "nohup nice -n 0 $NOTEBOOK_SCRIPT >>$LOG_FILE 2>&1 &"' echo $!' > $PIDFILE

    sleep 3

    checkstatusofproc
    RETVAL=$?
    [ $RETVAL -eq $STATUS_RUNNING ] && touch $LOCKFILE
    return $RETVAL
}


stop() {
    log_success_msg "Stopping $DESC (${DAEMON}): "
    killproc -p $PIDFILE ipython-notebook
    RETVAL=$?

    [ $RETVAL -eq $RETVAL_SUCCESS ] && rm -f $LOCKFILE $PIDFILE
    return $RETVAL
}


restart() {
  stop
  start
}


checkstatusofproc(){
  pidofproc -p $PIDFILE $PROC_NAME > /dev/null
}


checkstatus(){
  checkstatusofproc
  status=$?

  case "$status" in
    $STATUS_RUNNING)
      log_success_msg "${DESC} is running"
      ;;
    $STATUS_DEAD)
      log_failure_msg "${DESC} is dead and pid file exists"
      ;;
    $STATUS_DEAD_AND_LOCK)
      log_failure_msg "${DESC} is dead and lock file exists"
      ;;
    $STATUS_NOT_RUNNING)
      log_failure_msg "${DESC} is not running"
      ;;
    *)
      log_failure_msg "${DESC} status is unknown"
      ;;
  esac
  return $status
}


condrestart(){
  [ -e $LOCKFILE ] && restart || :
}


check_for_root() {
  if [ $(id -ur) -ne 0 ]; then
    echo 'Error: root user required'
    echo
    exit 1
  fi
}


set_mode()
{
    MODE=$1
    check_for_root
    test -f "/opt/spark/current/conf/spark-env.sh.$MODE" || { log_failure_msg "Error: config for $MODE missing. Probably needs set-addr"; return 1; }
    #test -f "${CFGNAME}.$MODE" || { log_failure_msg "can't find Spark config ${CFGNAME}.$MODE"; return 1; }
    stop
    #echo "$MODE" > ${CFGNAME}
    (cd /opt/spark/current/conf; \
	rm -f spark-env.sh spark-defaults.conf; \
	ln -s spark-env.sh.$MODE spark-env.sh; \
	ln -s spark-defaults.conf.$MODE spark-defaults.conf) 
    log_success_msg "Configured Spark for mode: $MODE "
    start
}


set_addr()
{
    CONF=/opt/spark/current/conf/
    MODE="$1"
    test -f "${CONF}tpl/spark-defaults.conf.$MODE.tpl" || { echo "can't find Spark config template for ${MODE}"; exit 1; }
    MASTER_IP="$2"
    NAMENODE_IP="$3"
    HISTORY_SERVER="$4"
    test "$NAMENODE_IP" || NAMENODE_IP=$MASTER_IP
    case "$NAMENODE_IP" in
	hdfs*) ;;
	*)  EVENTLOG_DIR="hdfs://$NAMENODE_IP:8020/user/spark/applicationHistory";;
    esac
    case "$HISTORY_SERVER" in
	*:*);;
	'') HISTORY_SERVER=${MASTER_IP}:18080;;
	*) HISTORY_SERVER=${HISTORY_SERVER}:18080;;
    esac
    # Modify Spark config accordingly
    for f in spark-defaults.conf spark-env.sh
    do
	sed -e "s@{HOSTNAME-MASTER}@$MASTER_IP@" \
	    -e "s@{HOSTNAME-NAMENODE}@$NAMENODE_IP@" \
	    -e "s@{HOSTNAME-SPARK-HS}@$HISTORY_SERVER@" \
	    -e "s@{DIR-EVENTLOG}@$EVENTLOG_DIR@" \
	    ${CONF}tpl/$f.$MODE.tpl > ${CONF}$f.$MODE
    done
    # If in YARN mode, update also Hadoop/Yarn config
    if [ "$MODE" = "yarn" ] ;then
	sed "s@{HOSTNAME-YARN-RM}@$MASTER_IP@" ${CONF}tpl/yarn-site.xml.tpl > ${CONF}hadoop/yarn-site.xml 
	sed "s@{HOSTNAME-NAMENODE}@$NAMENODE_IP@" ${CONF}tpl/core-site.xml.tpl > ${CONF}hadoop/core-site.xml 
    fi
}


usage() {
    echo $"Usage: $0 {start|stop|status|restart|try-restart|condrestart}"
    echo $"       $0 set-mode (local | standalone | yarn)"
    echo $"       $0 set-addr (standalone|yarn) <master-addr> [<namenode-addr> [<hserver-addr>]]"
    exit 1
}


service() {
  case "$1" in
    start)
      check_for_root
      start
      ;;
    stop)
      check_for_root
      stop
      ;;
    status)
      checkstatus
      RETVAL=$?
      ;;
    restart)
      check_for_root
      restart
      ;;
    condrestart|try-restart)
      check_for_root
      condrestart
      ;;
    set-mode)
      test "$2" = "local" -o "$2" = "yarn" -o "$2" = "standalone" || usage
      check_for_root
      set_mode $2
      RETVAL=$?
      ;;
    set-addr)
      check_for_root
      shift
      test "$1" = "yarn" -o "$1" = "standalone" || usage
      set_addr $@
      RETVAL=$?
      ;;
    *)
      usage
      ;;
  esac
}

service "$@"

exit $RETVAL
