# -*- mode: ruby;  encoding: iso-8859-1  -*-
# vi: set ft=ruby :
# **************************************************************************
# Create a CentOS 6 virtual machine with all the software needed to
# run Spark + notebooks
# **************************************************************************

vagrant_command = ARGV[0]

# Python version for which we will be creating the notebook environment
python_version = '3.12'

# --------------------------------------------------------------------------
# Variables defining the installation of elements

# The version of Spark we will install
spark_version = '3.5.4'
# [A] Install a locally available custom build
#spark_name = 'spark-' + spark_version + '-bin-custom'
# [B] Download & install a standard binary distribution
spark_name = 'spark-' + spark_version + '-bin-hadoop3'

# The place where Spark will be deployed inside the local machine
# There is usually no need to change this
spark_basedir = '/opt/spark'

# A local artifact repository where some custom-built files might be kept
# Currently not used
#repo_base = 'http://artifactory.hi.inet/artifactory/vagrant-machinelearning/buildfiles/'

# Virtualenv directory
venv_basedir = '/opt/ipnb'


# --------------------------------------------------------------------------
# Some variables that affect Vagrant execution

# Check the command requested
vagrant_command = ARGV[0]

# Scala kernel: Toree, SPylon, almond?
provision_run_skernel = ''

#provision_run_dl= true

# --------------------------------------------------------------------------
# Vagrant configuration

# The "2" in Vagrant.configure configures the configuration version
Vagrant.configure(2) do |config|

  # This is to help later when packaging: don't change the insecure key
  config.ssh.insert_key = false
  #config.ssh.forward_agent = true
  #config.ssh.username = "ubuntu"

  # vagrant-vbguest plugin: set auto_update to false, if you do NOT want to
  # check the correct additions version when booting this machine
  config.vbguest.auto_update = false

  config.vm.boot_timeout = 600

  config.vm.define "vm-spark-base64" do |vmconf|


    # The most common configuration options are documented and commented below.
    # For a complete reference, please see the online documentation at
    # https://docs.vagrantup.com.

    # The base box we will be using. 
    # Available at https://atlas.hashicorp.com/search
    vmconf.vm.box = "bento/ubuntu-24.04"
    vmconf.vm.box_version = "202404.26.0"
    #vmconf.vm.box_url = "file:///almacen/VM/Export/VagrantBox/spark-base64-3.1.2.box"

    # Disable automatic box update checking. If you disable this, then
    # boxes will only be checked for updates when the user runs
    # `vagrant box outdated`. This is not recommended.
    # vmconf.vm.box_check_update = false

    # Put into the usual synced folder a subdirectory
    vmconf.vm.synced_folder ".", "/vagrant", disabled: true
    vmconf.vm.synced_folder "vmfiles", "/vagrant",
    disabled: false
    # owner: spark_username
    #auto_mount: false
  
    # Customize the virtual machine: hostname & RAM
    vmconf.vm.hostname = "vgr-spark-base64"
    vmconf.vm.provider :virtualbox do |vb|
      # Set the hostname in the provider
      vb.name = vmconf.vm.hostname.to_s
      # Customize the amount of memory & CPUs for the VM
      #vb.memory = "1024"
      vb.memory = "7168"
      vb.cpus = '2'
      # Display the VirtualBox GUI when booting the machine
      #vb.gui = true
      # A patch for a problem in VirtualBox -- fixed in VB 5.0.28 and 5.1.6
      # see https://github.com/chef/bento/issues/688
      vb.customize ["modifyvm", :id, "--cableconnected1", "on"]
    end

    # Networking
    # Declare a public network
    #vmconf.vm.network "public_network", type: "dhcp", bridge: 'Realtek PCIe GBE Family Controller', :mac => "08002710A7ED"

    # Create a forwarded port mapping which allows access to a specific port
    # within the machine from a port on the host machine. In the example below,
    # accessing "localhost:8080" will access port 80 on the guest machine.
    #vmconf.vm.network "forwarded_port", guest: port_ipython, host: port_ipython
    #vmconf.vm.network :forwarded_port,
    #host: 4040, 
    #guest: 4040, 
    #auto_correct: true                 # Spark UI (Driver)

    # Share an additional folder to the guest VM. The first argument is
    # the path on the host to the actual folder. The second argument is
    # the path on the guest to mount the folder. And the optional third
    # argument is a set of non-required options.
    # vmconf.vm.synced_folder "../data", "/vagrant_data"

    # Define a Vagrant Push strategy for pushing to Atlas. Other push strategies
    # such as FTP and Heroku are also available. See the documentation at
    # https://docs.vagrantup.com/v2/push/atlas.html for more information.
    # vmconf.push.define "atlas" do |push|
    #   push.app = "YOUR_ATLAS_USERNAME/YOUR_APPLICATION_NAME"
    # end
    # Push to artifactory
    #curl -i -u<USERNAME>:<API_KEY> -T <PATH_TO_FILE> "http://artifactory.hi.inet:8081/artifactory/vagrant-machinelearning/{vagrantBoxName.box};box_name={name};box_provider={provider};box_version={version}"

    vmconf.vm.post_up_message = "**** The Vagrant Spark base machine is up"

    # ---------------------------------------------------------------------

    # https://github.com/mitchellh/vagrant/issues/1673
    vmconf.vm.provision "00.fix-tty",
    type: "shell",
    run: "never",
    privileged: true,
    inline: "sed -i '/tty/!s/mesg n/tty -s \\&\\& mesg n/' /root/.profile"
     
    # Install some base software
    # (inc. development environments we need to install some Python packages)
    vmconf.vm.provision "01.update",
    type: "shell",
    privileged: true,
    inline: <<-SHELL
     echo "Updating packages ..."
     apt-get update -y
     #DEBIAN_FRONTEND=noninteractive apt-get install -y keyboard-configuration
     DEBIAN_FRONTEND=noninteractive apt-get --yes --option Dpkg::Options::="--force-confdef" upgrade
    SHELL

    vmconf.vm.provision "02.base",
    type: "shell",
    privileged: true,
    inline: <<-SHELL

     # Install some basic packages + dev libraries to compile R/Python pkgs
     export DEBIAN_FRONTEND=noninteractive
     apt-get install -y build-essential
     apt-get install -y gcc g++ freetype-devel libpng-devel libffi-devel cmake
     apt-get install -y libopenblas0 libopenblas0-serial libopenblas64-0-serial
     apt-get install -y libopenblas-dev libopenblas-serial-dev
     # General utility programs
     apt-get install -y emacs-nox python3-venv graphviz

     # Make some subdirectories in the vagrant home dir
     sudo -iu vagrant mkdir bin install tmp

     # We don't use this
     systemctl disable ModemManager
    SHELL
    
    # Generate a few locales
    vmconf.vm.provision "03.locales",
    type: "shell",
    privileged: true,
    inline: <<-SHELL
     locale-gen "en_US.UTF-8" "es_ES.UTF-8" "pt_BR.UTF-8" "de_DE.UTF-8"
     dpkg-reconfigure -f noninteractive locales
    SHELL

    # .........................................
    vmconf.vm.provision "10.jdk",
    type: "shell",
    privileged: true, 
    inline: "apt-get install -y openjdk-11-jdk-headless"

    # Install R
    vmconf.vm.provision "11.R",
    type: "shell",
    privileged: true, 
    inline: <<-SHELL
     apt install --no-install-recommends software-properties-common dirmngr
     wget -qO- https://cloud.r-project.org/bin/linux/ubuntu/marutter_pubkey.asc | sudo tee -a /etc/apt/trusted.gpg.d/cran_ubuntu_key.asc
     add-apt-repository "deb https://cloud.r-project.org/bin/linux/ubuntu $(lsb_release -cs)-cran40/"
     apt-get update -y
     apt-get install -y r-recommended
     #(another option: r-base-dev)
    SHELL

    # Install some R packages
    vmconf.vm.provision "12.Rpkg",
    type: "shell",
    privileged: true,
    inline: <<-SHELL
     echo "Installing R packages"
     #RDIR=$(Rscript -e "for (p in .libPaths()) { if( startsWith(p,'/usr/local')) { cat(p); break; }}")
     #mkdir -p $RDIR
     #apt-get install -y pkg-config libpq-dev libcurl4-openssl-dev libzmq5 libmariadb-client-lgpl-dev libxml2-dev libnlopt-dev

     export DEBIAN_FRONTEND=noninteractive
     PKG="libxml2-dev libssl-dev libcurl4-openssl-dev libsodium-dev"
     apt-get install -y $PKG

     for pkg in "'rmarkdown'" \
                "'tidyverse'" \
                "'data.table'" \
                "'caret','neuralnet'"
     do
         echo -e "\nInstalling R packages: $pkg"
         Rscript -e "install.packages(c($pkg),dependencies=TRUE,repos=c('http://ftp.cixug.es/CRAN/','http://cran.es.r-project.org/'),quiet=FALSE)"
     done

     apt-get remove -y $PKG
     #libpq-dev libcurl4-openssl-dev libmariadb-client-lgpl-dev libxml2-dev libcurl4-gnutls-dev libnlopt-dev
    SHELL

    # Install the IR kernel for Jupyter & the sparklyr package
    vmconf.vm.provision "13.IRkernel",
    type: "shell",
    privileged: true,
    inline: <<-SHELL
     echo "Installing IRkernel"
     Rscript -e "install.packages('IRkernel',dependencies=TRUE,repos=c('http://ftp.cixug.es/CRAN/','http://cran.es.r-project.org/'),quiet=FALSE)"

     #Rscript -e 'install.packages( c("crayon","devtools"),repos=c("http://ftp.cixug.es/CRAN/","http://cran.es.r-project.org/"),quiet=FALSE)'
     #Rscript -e 'devtools::install_github( paste0("IRkernel/",c("repr","IRdisplay","IRkernel")) )'
     #Rscript -e 'devtools::install_github( "IRkernel/IRkernel" )'

     echo "Installing sparklyr"
     sudo apt-get install libxml2-dev libcurl4-gnutls-dev
     Rscript -e 'install.packages("sparklyr",dependencies=TRUE,repos=c("http://ftp.cixug.es/CRAN/","http://cran.es.r-project.org/"),quiet=FALSE)'
     apt-get remove -y libcurl4-gnutls-dev libxml2-dev
     #Rscript -e 'devtools::install_github("rstudio/sparklyr")'
    SHELL


    # .........................................
    # Create a Python virtualenv and install wrappers to point there
    vmconf.vm.provision "15.python.venv",
    type: "shell",
    privileged: false,
    args: [ python_version, venv_basedir ],
    inline: <<-SHELL
      VENV=$2
      PYVER=$1

      export DEBIAN_FRONTEND=noninteractive

      sudo apt-get install -y libpython$PYVER-dev

      # Prepare the place
      sudo rm -rf $VENV
      sudo mkdir -m 775 $VENV
      sudo chown vagrant:vagrant $VENV

      # Create a virtualenv
      python$1 -m venv  $VENV

      # Make links for vagrant user
      cd $HOME/bin
      rm -f python python$1 pip
      ln -s $VENV/bin/{python,python$PYVER,pip} .
    SHELL


    # .........................................
    # In the virtualenv, install the Python packages to use in the Notebooks
    # Plus a few ML-related more
    vmconf.vm.provision "16.python.pkg",
    type: "shell",
    keep_color: true,
    privileged: false,
    args: [ python_version, venv_basedir ],
    inline: <<-SHELL
      VENV=$2
      PYVER=$1

      sudo apt-get install -y libopenblas-dev libhdf5-dev libfreetype6-dev gsfonts libgraphviz-dev

      source $VENV/bin/activate
      pip install --upgrade pip
      pip install --upgrade setuptools
      pip install wheel build

      # we install with options to avoid InsecurePlatformWarning messages
      pip install 'requests[security]'

      pip install --upgrade pillow
      pip install --upgrade h5py
      pip install --upgrade tables
      pip install --upgrade hdfs
      pip install --upgrade openpyxl xlrd

      pip install --upgrade pyarrow
      pip install --upgrade pyparsing matplotlib
      pip install --upgrade pandas bottleneck numexpr

      pip install --upgrade pydot graphviz pygraphviz
      pip install --upgrade plotnine
      # vega, holoviews

      pip install --upgrade scikit-learn
      pip install --upgrade imbalanced-learn
      pip install --upgrade gensim
      pip install --upgrade networkx
      pip install --upgrade seaborn
      pip install --upgrade statsmodels
      pip install --upgrade pystan prophet pytz

      # this is a big package, and pulls in nvidia-nccl-cu12, also big
      #pip install --upgrade xgboost

      pip install --upgrade cloudpickle
      pip install --upgrade dask

      #https://discourse.jupyter.org/t/jupyter-notebook-zmq-message-arrived-on-closed-channel-error/17869/7
      pip install --upgrade jupyter-client==7.4.4 tornado==6.2

      pip install --upgrade ipython jupyter-console 'notebook>=7.1' nbclassic nbconvert
      (cd $VENV/lib/python$PYVER/site-packages/nbclassic/static/components/marked/lib; ln -s marked.umd.js marked.js)
      # https://github.com/ipython-contrib/jupyter_contrib_nbextensions/issues/1628
      #pip install --upgrade ipython jupyter-console nbconvert notebook "nbclassic<0.5"

      pip install --upgrade widgetsnbextension ipywidgets ipympl
      pip install --upgrade jupyter_nbextensions_configurator

      # We don't need this anymore
      sudo apt-get remove -y libhdf5-dev libfreetype-dev libfreetype6-dev \
           libgraphviz-dev libcurl4-openssl-dev libbrotli-dev libssl-dev \
           libncurses-dev

    SHELL

    # Build pylucene
    # Build as a wheel, because latest pip does not like eggs
    # (https://github.com/pypa/pip/issues/12330)
    # https://github.com/hscells/pylucene-packaged/blob/main/.github/workflows/package.yml
    vmconf.vm.provision "17.pylucene",
    type: "shell",
    keep_color: true,
    privileged: false,
    args: [ venv_basedir ],
    inline: <<-SHELL
    VENV=$1

    mkdir -p Soft
    cd Soft
    VERSION=9.12.0
    PKG=pylucene-$VERSION
    PKGFILE=$PKG-src.tar.gz
    rm -rf $PKG $PKGFILE
    wget --no-verbose http://apache.rediris.es/lucene/pylucene/$PKGFILE || exit 1
    tar zxvf $PKGFILE
    cd $PKG

    # Patch the Makefile to add the phonetic analysis classes to Pylucene
    patch -p0 < /vagrant/lucene/$PKG.patch || exit 1

    source $VENV/bin/activate

    # Build & install JCC -- as a wheel, not as an egg
    cd jcc
    export JCC_JDK=$(type -p javac|xargs readlink -f|xargs dirname|xargs dirname)
    export JCC_ARGSEP=" "
    export JCC_LFLAGS="-L${JCC_JDK}/lib -ljava -L${JCC_JDK}/lib/server -ljvm -Wl,-rpath=${JCC_JDK}/lib:${JCC_JDK}/lib/server"
    python setup.py build && pip install . || exit 1
    cd ..

    # build & install pylucene (also as a wheel)
    sudo apt-get install -y ant
    export PREFIX_PYTHON="$VENV"
    export PYTHON="${PREFIX_PYTHON}/bin/python3"
    export JCC="$PYTHON -m jcc --shared --wheel"
    export ANT="JAVA_HOME=$JCC_JDK /usr/bin/ant"
    export NUM_FILES=16
    make sources && make lucene && make compile && pip install dist/lucene-$VERSION-cp312-cp312-linux_x86_64.whl

    SHELL

    vmconf.vm.provision "18.cartopy",
    type: "shell",
    keep_color: true,
    privileged: false,
    inline: <<-SHELL
      sudo apt-get -y install libgeos-dev
      pip install cartopy
      sudo apt-get -y remove libgeos-dev
    SHELL

    # .........................................
    # Install a pre-built Spark

    vmconf.vm.provision "30.spark",
    type: "shell",
    privileged: false,
    keep_color: true,
    args: [ spark_version, spark_name, spark_basedir ],
    inline: <<-SHELL

      # download & install Spark
      sudo bash -c "mkdir -p -m 775 '$3'; chown vagrant.vagrant '$3'; cd $3; rm -f current; ln -s $2/ current"
      case $2 in
        *custom) 
           file="/vagrant/spark/$2.tgz";;
        *) echo "Downloading $2.tgz"
           cd install
           wget --no-verbose "http://dlcdn.apache.org/spark/spark-$1/$2.tgz" || exit 1
           file=$2.tgz;;
      esac
      tar zxvf "$file" -C "$3"
      sudo sh -c "echo 'PATH=\\$PATH:$3/current/bin' > /etc/profile.d/spark-path.sh"

      # Create the directory to place Hadoop config
      mkdir -p "$3/current/conf/hadoop"
      sudo rm -f /etc/hadoop
      sudo ln -s "$3/current/conf/hadoop" /etc/hadoop

      # Ensure we will be able to write to the Notebook log & run directories
      for d in /var/log /var/run; do
        test -d $d/ipnb || sudo mkdir -m 1777 $d/ipnb
      done

      # Add to the R library directory a link to the installed SparkR package
      RDIR=$(Rscript -e "for (p in .libPaths()) { if( startsWith(p,'/usr/local')) { cat(p); break; }}")
      if [ -d $RDIR ]; then
         sudo rm -f "$RDIR/SparkR"
         sudo ln -s "$3/$2/R/lib/SparkR/" "$RDIR"
      fi
    SHELL

    # .........................................
    # Spark add-ons to install
    #  * GraphFrames snapshot (if applicable)
    #  * Hadoop Brotli codec
    # (Kafka Structured Streaming can be declared as package in
    # Spark config and it will be automatically downloaded)
    # .........................................

    # Install the brotli codec
    vmconf.vm.provision "31.spark.pkg",
    type: "file",
    source: "buildfiles/extra",
    destination: spark_basedir + "/" + spark_name + "/jars/extra"


    # Install icons for Pyspark & Scala kernels
    vmconf.vm.provision "32.kernel.icons",
    type: "file", 
    source: 'buildfiles/style/kernel-icons',
    destination: spark_basedir + '/kernel-icons'

    
    # .........................................
    # Install the Spark configuration files for the different modes

    # Move all original system config to "orig"
    vmconf.vm.provision "32a.spark.cfg",
    type: "shell",
    privileged: false,
    keep_color: true,
    args: [ spark_basedir ],
    inline: <<-SHELL
      cd $1/current/conf
      mkdir -p orig
      mv *.template orig
    SHELL

    # Copy local config
    vmconf.vm.provision "32b.spark.cfg",
    type: "file",
    source: 'buildfiles/conf',
    destination: spark_basedir + '/current/conf'

    # Prepare a configuration
    vmconf.vm.provision "32c.spark.cfg",
    type: "shell",
    privileged: false,
    keep_color: true,
    args: [ spark_basedir ],
    inline: <<-SHELL
      cd $1/current/conf
      # Install thge default config
      ln -s opt/spark-defaults.conf.local spark-defaults.conf
      ln -s opt/spark-env.sh.local spark-env.sh
      # Prepare a config for GraphFrames
      for f in spark-env.sh spark-defaults.conf
      do
         sed 's|#GFP#||g' opt/${f}.local > opt/${f}.graphframes
      done
    SHELL

    # .........................................

    # Install the Systemd unit file to manage the notebook server
    vmconf.vm.provision "36a.nbk.systemd",
    type: "file",
    source: "buildfiles/startup/notebook.service",
    destination: "/tmp/notebook.service"

    vmconf.vm.provision "36b.nbk.systemd",
    type: "shell",
    privileged: true,
    inline: 'K=notebook.service; D=/etc/systemd/system; mv /tmp/$K $D; chown root:root $D/$K'

    # .........................................
    # Install the IPython Spark notebook daemon script
    vmconf.vm.provision "40a.nbk.script",
    type: "file",
    source: "buildfiles/startup/jupyter-notebook-mgr",
    destination: venv_basedir + "/bin/jupyter-notebook-mgr"

    vmconf.vm.provision "40b.nbk.wrapper",
    type: "file",
    source: "buildfiles/startup/jupyter-notebook-wrapper",
    destination: venv_basedir +"/bin/jupyter-notebook-wrapper"

    # Install the Pyspark script wrapper
    vmconf.vm.provision "41.pyspark.wrapper",
    type: "file",
    source: "buildfiles/startup/pyspark-ipnb",
    destination: venv_basedir +"/bin/pyspark-ipnb"

    # .........................................
    # Install some files for notebooks

    # a custom css
    vmconf.vm.provision "42.nbk.css",
    type: "file",
    source: "buildfiles/style/notebook-custom.css",
    destination: "#{venv_basedir}/lib/python#{python_version}/site-packages/notebook/static/custom/custom.css"

    # install notebook extensions
    vmconf.vm.provision "43a.nbk.ext",
    type: "shell",
    keep_color: true,
    privileged: false, 
    args: [ venv_basedir ],
    inline: <<-SHELL
      VENV=$1
      source $VENV/bin/activate

      # Install & enable ipywidgets extension
      jupyter nbclassic-extension install --py widgetsnbextension --sys-prefix
      jupyter nbclassic-extension enable --py widgetsnbextension --sys-prefix

      # Install the extensions configurator and enable it
      #jupyter nbextensions_configurator enable --sys-prefix
      jupyter nbclassic-extension install --py jupyter_nbextensions_configurator --sys-prefix
      jupyter nbclassic-extension enable --py jupyter_nbextensions_configurator --sys-prefix

      # Install the contrib nbextensions
      #jupyter contrib nbextension install --sys-prefix
    SHELL

    # add some custom notebook extensions
    # (including a patch for toc2)
    vmconf.vm.provision "43b.nbk.ext",
    type: 'file',
    source: 'buildfiles/nbextensions',
    destination: venv_basedir + "/share/jupyter/nbextensions"

    # enable desired extensions
    vmconf.vm.provision "43c.nbk.ext",
    type: "shell",
    keep_color: true,
    privileged: false,
    args: [ venv_basedir ],
    inline: <<-SHELL
      VENV=$1
      source $VENV/bin/activate
      for ext in toc2 toggle-headers search-replace python-markdown
      do
        jupyter nbclassic-extension enable ${ext}/main --sys-prefix
      done
    SHELL

    # a slightly modified Jupyter icon
    # (so that it's easier to identify browser tabs belonging to the VM)
    vmconf.vm.provision "44.icon.jupyter",
    type: "file",
    source: 'buildfiles/style/jupyter-favicon-custom.ico',
    destination: "#{venv_basedir}/lib/python#{python_version}/site-packages/notebook/static/base/images/favicon-custom.ico"


    # .........................................
    # message of the day
    vmconf.vm.provision "50a.motd",
    type: "file",
    source: 'buildfiles/motd',
    destination: '/tmp/'

    vmconf.vm.provision "50b.motd",
    type: "shell",
    privileged: true,
    inline: 'cp -p /tmp/motd/* /etc/update-motd.d; rm -f /etc/update-motd.d/99-bento'

    # .........................................
    # Install a Scala Jupyter kernel
    if (provision_run_skernel == 'toree')
    vmconf.vm.provision "60.scala.kernel",
    type: "shell",
    privileged: false,
    keep_color: true,
    args: [ spark_version, spark_name, spark_basedir, repo_base ],
    inline: <<-SHELL
      PKG=toree-0.4.0
      echo "Installing the (Scala) Toree Spark kernel $PKG"
      # -- [A] From PyPi
      pip install toree
      # -- [B] From the developer snapshot
      #pip install https://dist.apache.org/repos/dist/dev/incubator/toree/0.2.0/snapshots/dev1/toree-pip/$PKG
      # -- [C] From a local repository
      #cd $HOME/install
      #wget --no-verbose "${4}spark-kernel/$1/$PKG"
      #pip install $PKG
      # -- [D] From a locally available file
      #pip install /vagrant/toree/$PKG 
    SHELL
    elsif (provision_run_skernel == 'spylon')
    vmconf.vm.provision "60.scala.kernel",
    type: "shell",
    privileged: false,
    keep_color: true,
    inline: <<-SHELL
      echo "Installing the (Scala) SPylon kernel"
      pip install spylon_kernel
    SHELL
    elsif (provision_run_skernel == 'almond')
    vmconf.vm.provision "60.scala.kernel",
    type: "shell",
    privileged: false,
    keep_color: true,
    inline: <<-SHELL
       curl -Lo coursier https://git.io/coursier-cli
       chmod +x coursier
       ./coursier launch --fork almond --scala 2.12.13 -- --install
       rm -f coursier
     SHELL
    end

    # .........................................
    vmconf.vm.provision "70.dl",
      type: "shell",
      privileged: false,
      run: "never",
      keep_color: true,
      inline: <<-SHELL
         pip install --upgrade tensorflow-cpu tensorboard
         pip install --upgrade torch torchvision
       SHELL
    
    # .........................................
    # Install Scala
    vmconf.vm.provision "71.scala",
      type: "shell",
      privileged: false,
      run: "never",
      keep_color: true,
      args: [ spark_version, spark_name, spark_basedir ],
      inline: <<-SHELL
        # download & install Scala
        cd install
        VERSION=2.12.11
        echo "Downloading Scala $VERSION"
        wget --no-verbose http://downloads.lightbend.com/scala/$VERSION/scala-$VERSION.deb
        sudo dpkg -i scala-$VERSION.deb
      SHELL

    # .........................................
    # Clean
    vmconf.vm.provision "99.clean",
      type: "shell",
      run: "never",
      privileged: true,
      inline: <<-SHELL
        echo "*** Removing snaps"
        for s in lxd core20 snapd
        do
            snap remove $s
        done

        echo "*** Cleaning temporal & installation files"
        apt-get autoclean -y
        apt-get clean -y
        apt-get remove -y libx11-dev ibgdk-pixbuf2.0-dev libice-dev libxpm-dev \
                          libpixman-1-dev libpthread-stubs0-dev libxt-dev \
                          libxcb-shm0-dev libxcb1-dev libxdmcp-dev libxext-dev \
                          x11proto-core-dev x11proto-input-dev x11proto-kb-dev \
                          x11proto-render-dev x11proto-xext-dev xtrans-dev \
                          krb5-multidev libaec-dev libjbig-dev libtiff5-dev \
                          libvpx-dev libx11-doc libxau-dev swig3.0 x11proto-dev

        # --- don't do this -- we may remove something needed (because it is
        # used by a program that was compiled, so no dependencies declared)
        #apt-get autoremove -y --purge

        #this will need to be done by hand
        #dpkg --purge linux-image-4.4.0-81-generic linux-image-extra-4.4.0-81-generic

        # Remove bash history for root
        unset HISTFILE
        rm -f /root/.bash_history

        # Remove resume information
        echo "RESUME=none" > /etc/initramfs-tools/conf.d/resume
        apt purge mdadm

        # Cleanup log files
        echo "*** Removing logfiles"
        find /var/log -type f | while read f; do echo -ne '' > $f; done;
        journalctl --rotate --vacuum-size=10K

        # Remove leftovers for vagrant user
        HH=/home/vagrant
        rm -rf $HH/.cache/pip $HH/.gradle $HH/.bash_history $HH/*.log
        rm -rf $HH/.m2 $HH/.ivy2 $HH/.ant $HH/.sbt $HH/.emacs-d/eln-cache
        rm -rf $HH/install/* $HH/Soft $HH/tmp/*
        rm -rf /opt/ipnb/share/jupyter/nbextensions/.git*

        # Remove all temporal files
        echo "*** Cleaning temporal files"
        rm -rf /tmp/* /var/tmp/*

        # Remove swapfile
        SWAPFILE=/swap.img
        if [ -f $SWAPFILE ]
        then
               echo "*** Removing swapfile"
               sed --in-place=.swap "s@^$SWAPFILE@#$SWAPFILE@" /etc/fstab
               swapoff $SWAPFILE
               rm $SWAPFILE
        fi

        # Zero swapfiles/swap partitions
        cat /proc/swaps | tail -n +2 | awk -F ' ' '{print $1}' | while read SW
        do
            echo "*** Whiteout swap space: $SW"
            swapoff $SW;
            case $SW in
              /dev/*) dd if=/dev/zero of=$SW bs=1024;;
                   *) shred -n 0 -z $SW;;
            esac
            mkswap $SW && swapon $SW
        done

        # Zero disk free space
        echo "*** Whiteout root & boot partitions"
        for fs in / /boot/
        do
           echo "    Partition: $fs"
           count=$(df --sync -kP $fs | tail -n1 | awk -F ' ' '{print $4}')
           let count--
           dd if=/dev/zero of=${fs}whitespace bs=1024 count=$count
           rm -f ${fs}whitespace;
        done
        sync

      SHELL

  end

end
