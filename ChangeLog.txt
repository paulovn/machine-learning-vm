v. 3.4.0
 * Spark is 3.4.1
 * Updated some Python packages, notably notebook
 * Updated sparklyr
 * Graphframes is 0.8.3 (official, to be downloaded from repository at runtime),
   0.8.4-SNAPSHOT also available in the VM

v. 3.3.0
 * Base OS updated to Ubuntu 22.04
   - R is now 4.x
   - Python is 3.10.x
 * Spark is 3.3.2
 * Graphframes 0.8.3-SNAPSHOT assembly jar added to the VM

v. 3.1.2
 * Base OS Updated to Ubuntu 20.04
   - R is now 4.0.3
   - Python is 3.8.5
 * Spark is 3.1.2

v. 3.0.0
 * Base OS Updated to Ubuntu 20.04
   - R is now 4.0.3
   - Python is 3.8.5
 * Spark is 3.1.1
 * PyLucene is 8.6.1

v. 2.2.2
 * Spark is 2.4.6
 * Base OS Updated (R is now 3.6.2)
 * Updated some Python packages. PyLucene is now 8.1.1
 * Added package for Brotli codec (to use in Parquet files)
 * Updated Spark config
    - Use the optimized version of Hadoop FileOutputCommiter
    - Packages for Spark 2.4.6
    - Compatibiliy Setting for PyArrow >= 0.15.0
 * Refactored management scripts
    - Start jupyter service automatically at system startup
    - Wrap Jupyter actual launch to wait until the notebook directory (shared folder)
      is ready


v. 2.2.1
 * Spark is 2.4.0
 * Updated Graphframes and Kakfa Streaming packages for 2.4.0
 * Base OS updated (Python is now 3.6.7, R is 3.5.2)
 * Added Python package pillow
 * Notebook management script and Python wrapper updated to name applications
   with numbers in sequence

v. 2.2.0
 * Based on Ubuntu 18.04
 * Uses Python 3.6
 * Spark is 2.3.2 (custom built, to include native BLAS bindings)
 * Python packages updated to latest version. Added jupyterlab
 * PyLucene updated to 7.5.0
 * Graphframes 0.6.0 release
 * Added gsfonts to the base image (graphviz needs it)
 * R 3.5.1

v. 2.1.0
 * Spark is 2.3.0 (custom built, to include native BLAS bindings)
 * Added Cartopy
 * Added PyLucene
 * Python packages updated to latest version. Installed Seaborn from github master
 * pydot-ng changed for pydot
 * GraphViz compiled from source, to allow the use of "sfdp" engine
 * GraphFrames updated to latest 0.6.0 snapshot
 * Set PYSPARK_DRIVER_PYTHON to ipython, but only if not set (by e.g. Jupyter)
 * R 3.4.4

v. 2.0.1
 * Based on Ubuntu 16.04
 * Python is version 3.5
 * Spark is 2.2.0 (custom built, to include native BLAS bindings)
 * IPython 6
 * R 3.4.2
 * Use SPylon for Scala kernel
 * Add a custom GraphFrames 0.6.0 snapshot build, with suitable configuration

v. 1.9.7
 * Improved cleanup for box size reduction
 
v. 1.9.6
 * Spark is 2.1.0
 * based on CentOS 7.3
 * IPython 5.3.0
 * R 3.3.2
 * use OpenJDK 8 instead of Oracle Java
 * no Theano/Keras installation (left to child boxes),
 * fixes in manager script
 * custom build of Spark, including native BLAS bindings
 * sparklyr installed from github
 * custom Toree build
 
v. 1.9.3
 * Spark is 2.0.2
 * added sparklyr
 * notebook daemon started as Systemd service
 * theanorc modified for explicit linking to openblas

v. 1.9.2
 * Spark is 2.0.1
 * Uses OpenJDK 1.8 instead of Oracle Java 8
 * R installation refactored to avoid pulling in Java 1.7

v. 1.9.1
 * Spark is 2.0.0
 * R is 3.3.1
 * Based on CentOS 7.2 (w/ Python 2.7.5)
 * No Python SCL installation (uses system Python 2.7)
 * IPython 5.1.0
 * Removed Spark add-ons (csv is built-in, graphframes can be downloaded on
   the fly)

v. 1.0.0

v. 0.9.13
 * Spark updated to 1.6.2
 * Added graphviz Python package
 * Added a system theanorc file
 * Install openblas
 * Updated to the most recent available versions of Python & R packages
 * Reduced Spark log levels

v. 0.9.12
 * R updated to 3.3.0
 * Python updated to 2.7.8
 * bugfix in notebook service script
 * Updated Toree kernel

v. 0.9.11
 * bugfixes

v. 0.9.10
 * Spark is 1.6.1
 * Spark Kernel changed to Toree
 * Added additional Python packages (Theano, Keras, Openpyxl, xlrd, h5py,
   tables)
 * R updated to 3.2.3
 * Added additional R packages: caret
 * Configured to allow 2 Python kernal: a base 2.7 and a Pyspark kernel
 * Added a couple of additional notebook extensions

v. 0.9.8
 * added a missing dependency for spark-csv
 * config option for extra path to include spark-csv (commented out)

v. 0.9.7

 * Spark is 1.6.0
 * Added configuration of Spark mode + cluster addr via script
 * Default network mode is bridged mode, to enable access to remote execution
 * Added additional jars (Kafka Spark Streaming, Spark CSV)

v. 0.9.5

 * Spark updated to 1.5.2
 * Add a slightly modified Jupyter icon
 * Updated Java 8 (to JDK 8u65)
 * Added more Pyhon libraries to virtualenv (statmodels, mpld3, seaborn, hdfs)
 * Added Spark/Scala notebook kernel, for Scala 2.10.5
     * added scala icon
 * Notebook extensions
     * modified ToC nbextension (suppress numbers in already numbered headings)
     * add search-replace nbextension
     * add toggle-headers nbextension
 * bugfix: added chown to spark log file in daemon script


v. 0.9.4

 * Start with a Centos 6.7 box
 * Download all needed support packages: Java 8, R, Python 2.7
 * Create a Python virtualenv & download Python libraries for scientific computing, inc. Jupyter notebooks
 * Add some R packages, including IRkernel
 * Download & install Spark 1.5.0
 * Download & install Scala 2.10
 * Install a few other goodies

The resulting Spark installation is pre-configured, but not yet started 
in the VM.
