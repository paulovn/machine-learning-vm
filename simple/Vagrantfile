# -*- mode: ruby;  ruby-indent-tabs-mode: t -*-
# vi: set ft=ruby :
# **************************************************************************
# Take the base box and remove Spark & associated stuff to creater a leaner base
# **************************************************************************

# --------------------------------------------------------------------------
# Variables defining the configuration of Spark & notebook 
# Modify as needed

# RAM memory used for the VM, in MB
vm_memory = '1024'
# Number of CPU cores assigned to the VM
vm_cpus = '1'


# --------------------------------------------------------------------------
# Some variables that affect Vagrant execution
# Check the command requested -- if ssh we'll change the login user
vagrant_command = ARGV[0]

# Conditionally activate some provision sections
provision_run_clean  = ENV['PROVISION_CLEAN'] == '1' || \
        (vagrant_command == 'provision' && ARGV.include?('80.clean'))


# --------------------------------------------------------------------------
# Vagrant configuration

# The "2" in Vagrant.configure sets the configuration version
Vagrant.configure(2) do |config|

  # This is to avoid Vagrant inserting a new SSH key, instead of the
  # default one (perhaps because the box will be later packaged)
  config.ssh.insert_key = false

  config.vm.define "vm-ml-nb64" do |vgrspark|

    # The base box we are using. As fetched from ATLAS
    vgrspark.vm.box_version = "= 2.1.0"
    vgrspark.vm.box = "paulovn/spark-base64"

    # Alternative place: UAM internal
    #vgrspark.vm.box = "uam/spark-base64"
    #vgrspark.vm.box_url = "http://svrbigdata.ii.uam.es/vm/uam-spark-base64.json"
    # Alternative place: TID internal
    #vgrspark.vm.box = "tid/spark-base64"
    #vgrspark.vm.box_url = "http://artifactory.hi.inet/artifactory/vagrant-machinelearning/tid-spark-base64.json"
    # Alternative place: local box
    vgrspark.vm.box_url = "file:///almacen/VM/VagrantBox/spark-base64-LOCAL.json"

    # Disable automatic box update checking. If you disable this, then
    # boxes will only be checked for updates when the user runs
    # `vagrant box outdated`. This is not recommended.
    # vgrspark.vm.box_check_update = false

    # Customize the virtual machine: set hostname & allocated RAM
    vgrspark.vm.hostname = "vm-ipnb-simple"
    vgrspark.vm.provider :virtualbox do |vb|
      # Set the hostname in VirtualBox
      vb.name = vgrspark.vm.hostname.to_s
      # Customize the amount of memory on the VM
      vb.memory = vm_memory
      # Set the number of CPUs
      vb.cpus = vm_cpus
      # Display the VirtualBox GUI when booting the machine
      #vb.gui = true
    end

    # **********************************************************************
    # Networking

    # ---- bridged interface ----
    # Declare a public network
    # This enables the machine to be connected from outside, which is a
    # must for a Spark driver [it needs SPARK_LOCAL_IP to be set to 
    # the outside-visible interface].
    # =====> Uncomment the following two lines to enable bridge mode:
    #vgrspark.vm.network "public_network",
    #type: "dhcp"

    # ===> if the host has more than one interface, we can set which one to use
    #bridge: "wlan0"
    # ===> we can also set the MAC address we will send to the DHCP server
    #:mac => "08002710A7ED"


    # ---- private interface ----
    # Create a private network, which allows host-only access to the machine
    # using a specific IP.
    #vgrspark.vm.network "private_network", ip: "192.72.33.10"



    # **********************************************************************
    # Provisioning: install Spark configuration files and startup scripts

    # .........................................
    # Create the user to run Spark jobs (esp. notebook processes)
    vgrspark.vm.provision "01.remove-spark",
    type: "shell", 
    privileged: true,
    inline: <<-SHELL
      rm -rf /opt/spark
      rm -f /usr/local/lib/R/library/SparkR
    SHELL

    vgrspark.vm.provision "02.remove-jdk",
    type: "shell", 
    privileged: true,
    inline: <<-SHELL
      apt-get remove -y openjdk-8-jdk-headless
    SHELL
    
    vgrspark.vm.provision "03.clean-virtualenv",
    type: "shell", 
    privileged: false,
    inline: <<-SHELL
      pip uninstall -y spylon_kernel
      pip install future
      rm -f /opt/ipnb/bin/pyspark-ipnb
    SHELL

    
    if (provision_run_clean)
      vgrspark.vm.provision "80.clean",
      type: "shell",
      privileged: true,
      inline: <<-SHELL
        echo "Cleaning temporal & installation files"
        apt-get autoclean -y
        apt-get clean -y

        # Remove bash history for root
        unset HISTFILE
        rm -f /root/.bash_history

        # Cleanup log files
        echo "Removing logfiles"
        find /var/log -type f | while read f; do echo -ne '' > $f; done;

        # Remove all temporal files
        rm -rf /tmp/* /var/tmp/*

        # Remove leftovers
        HH=/home/vagrant
        rm -rf $HH/.cache/pip $HH/.bash_history $HH/install/* $HH/Soft
        rm -rf /opt/ipnb/share/jupyter/nbextensions/.git*

        # Zero free space
        echo "Whiteout root & boot partitions"
        for fs in / /boot/
        do
           count=$(df --sync -kP / | tail -n1  | awk -F ' ' '{print $4}') 
           let count--
           dd if=/dev/zero of=${fs}whitespace bs=1024 count=$count
           rm ${fs}whitespace;
        done
        sync

        # Zero the swap space
        swappart=$(cat /proc/swaps | tail -n1 | awk -F ' ' '{print $1}')
        if [ "$swappart" != "" ]; then
          swapoff $swappart;
          dd if=/dev/zero of=$swappart;
          mkswap $swappart;
          swapon $swappart;
        fi

      SHELL
    end

  end # config.vm.define

end
